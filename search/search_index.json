{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Computer Knowledge Database Getting Started For most of the examples, I've provided a Makefile to make it easy to jump in. Make sure you have make installed by running the following command: sudo apt install -y make Then you can move into a particular directory, and run all of the unit tests with the command: make test For example, if you wanted to run the example for djikstra's algorithm written in python: cd data_structures_and_algorithms/graphs/dijkstra/python make test","title":"Home"},{"location":"#computer-knowledge-database","text":"","title":"Computer Knowledge Database"},{"location":"#getting-started","text":"For most of the examples, I've provided a Makefile to make it easy to jump in. Make sure you have make installed by running the following command: sudo apt install -y make Then you can move into a particular directory, and run all of the unit tests with the command: make test For example, if you wanted to run the example for djikstra's algorithm written in python: cd data_structures_and_algorithms/graphs/dijkstra/python make test","title":"Getting Started"},{"location":"data_structures_and_algorithms/","text":"Data Structures and Algorithms Note: A significant portion of the DSA information contained within this repository is adopted from the book Grokking Algorithms . While several of the coding samples in this repo follow the book very closely, others are my own (sometimes hacky) implementations. Table of Contents Selection Sort Recursion Quicksort Hash Tables Breadth First Search Dijkstra's Algorithm Greedy Algorithms Dynamic Programming Common Big O Run times O(log n): Logarithmic time, binary search O(n): Linear time, simple search O(n * log n) : Fast sorting algorithms O(n^2) : Slow sorting algorithms, bubble sort O(n!) : Factorial, traveling salesman PlantUML Diagrams Install Pre-requisites # Install Java sudo apt install default-jre # Install Graphvis sudo apt install graphviz # Download PlantUML jar to ~/.local/bin/plantuml.jar curl -L -o ~/.local/bin/plantuml.jar https://sourceforge.net/projects/plantuml/files/plantuml.jar/download # Check that PlantUML is installed correctly java -jar ~/.local/bin/plantuml.jar # Install feh to view output PNG sudo apt install feh # Alternative: Use water-uml to render a live view sudo npm i -g water-plant-uml Generate Diagrams from puml file # Render a PUML file into a PNG java -jar ~/.local/bin/plantuml.jar exampleDiagram.puml # View the PNG feh exampleDiagram.png # Live View: water-uml live exampleDiagram.puml","title":"Overview"},{"location":"data_structures_and_algorithms/#data-structures-and-algorithms","text":"Note: A significant portion of the DSA information contained within this repository is adopted from the book Grokking Algorithms . While several of the coding samples in this repo follow the book very closely, others are my own (sometimes hacky) implementations.","title":"Data Structures and Algorithms"},{"location":"data_structures_and_algorithms/#table-of-contents","text":"Selection Sort Recursion Quicksort Hash Tables Breadth First Search Dijkstra's Algorithm Greedy Algorithms Dynamic Programming","title":"Table of Contents"},{"location":"data_structures_and_algorithms/#common-big-o-run-times","text":"O(log n): Logarithmic time, binary search O(n): Linear time, simple search O(n * log n) : Fast sorting algorithms O(n^2) : Slow sorting algorithms, bubble sort O(n!) : Factorial, traveling salesman","title":"Common Big O Run times"},{"location":"data_structures_and_algorithms/#plantuml-diagrams","text":"","title":"PlantUML Diagrams"},{"location":"data_structures_and_algorithms/#install-pre-requisites","text":"# Install Java sudo apt install default-jre # Install Graphvis sudo apt install graphviz # Download PlantUML jar to ~/.local/bin/plantuml.jar curl -L -o ~/.local/bin/plantuml.jar https://sourceforge.net/projects/plantuml/files/plantuml.jar/download # Check that PlantUML is installed correctly java -jar ~/.local/bin/plantuml.jar # Install feh to view output PNG sudo apt install feh # Alternative: Use water-uml to render a live view sudo npm i -g water-plant-uml","title":"Install Pre-requisites"},{"location":"data_structures_and_algorithms/#generate-diagrams-from-puml-file","text":"# Render a PUML file into a PNG java -jar ~/.local/bin/plantuml.jar exampleDiagram.puml # View the PNG feh exampleDiagram.png # Live View: water-uml live exampleDiagram.puml","title":"Generate Diagrams from puml file"},{"location":"data_structures_and_algorithms/binary_search/","text":"Binary Search Overview Binary search is the simple algorithm we use when looking for an item in a sorted set: Find a word in the dictionary Playing a number guessing game (too high / too low) Complexity Binary search has a Logarithmic log(n) time complexity. The intuition is that each search halves the remaining search space. Notes Binary search is much faster than a simple search","title":"Binary Search"},{"location":"data_structures_and_algorithms/binary_search/#binary-search","text":"","title":"Binary Search"},{"location":"data_structures_and_algorithms/binary_search/#overview","text":"Binary search is the simple algorithm we use when looking for an item in a sorted set: Find a word in the dictionary Playing a number guessing game (too high / too low)","title":"Overview"},{"location":"data_structures_and_algorithms/binary_search/#complexity","text":"Binary search has a Logarithmic log(n) time complexity. The intuition is that each search halves the remaining search space.","title":"Complexity"},{"location":"data_structures_and_algorithms/binary_search/#notes","text":"Binary search is much faster than a simple search","title":"Notes"},{"location":"data_structures_and_algorithms/dynamic_programming/","text":"Dynamic Programming Dynamic Programming is used for optimization problems that involve a constraint . Dynamic Programming is a technique for solving hard problems, it's not a one-size-fits-all algorithm. It only works on problems that can be broken into discrete sub-problems . Overview Every Dynamic Programming algorithm uses a grid to keep track of the results of the subproblem. Break a large problem into smaller subproblems (sort of like recursion) Store the results from each of the subproblems in the grid Build the final solution up by re-using the already-computed results of the subproblems The Knapsack Problem You are a thief. You can only steal the things you can fit in your bag. You see these items that you might steal: Item Value Weight Stereo $3000 4 lbs Laptop $2000 3 lbs Guitar $1500 1 lbs Toaster $500 1 lbs You want to steal as much value as you can, but your bag only carries 4 lbs ! What should we take? So you can steal the Stereo for $3000, but that takes up all of your 4 lb bag space. You could take the Laptop for $2000, which uses 3 lbs of your bag space. That leaves us with 1 lb of leftover space . In that leftover space we could take the toaster or the guitar. It's obvious to us that we'd take the guitar, it's more valuable. This is the part that we should focus on: What question are we really answering here? In that leftover space we could take the toaster or the guitar. It's obvious to us that we'd take the guitar, it's more valuable. We can rephrase this question as What is the maximum value we can put in 1 lb of leftover space? But what if we had 2 lbs of leftover space ? We can't just multiply the 1 lb answer by 2, there is only one guitar that we can steal! So there is probably a different answer for all of the leftover space options: 1lb, 2lb, 3lb. Then, when we're deciding whether to take the next item, we have the current max for each of those bucket sizes. If we have leftover space, we have a pre-computed maximum value that we can fit in that space! The Knapsack DP Solution First, we'll setup the grid . Each column represents the solution to the subproblem where bag-size is that column number. Each row is a potential item to steal. We have a bag of size 4 , so our right-most column will have the value 4lbs . The smallest item weighs 1lbs , this will be the size of our smallest bucket & the increments between buckets. 1lb 2lb 3lb 4lb ... ... ... ... ... ... ... ... ... ... Now we can add the rows, one for each item: 1lb 2lb 3lb 4lb Stereo Laptop Guitar Toaster First we'll solve the stereo row: The stereo won't fit in any of the bags with weights 1 through 3, and we don't know anything better to put in there yet. It will fit in the 4lb slot though, so we'll mark that one. 1lb 2lb 3lb 4lb Stereo 0 0 0 3000 {S} Laptop Guitar Toaster Now we can solve the Laptop Row . It won't fit in the 1lb or 2lb colums. It will fit in the 3lb slot, and we haven't seen a better deal yet, so we'll mark it there. When we get to the 4lb slot, we can choose to take the previous best ( the cell directly above us, a $3000 stereo ). Or we can craft a solution with the current item, the Laptop, which is 3lbs . If we took the laptop, we'd have 1lb of free space leftover that we could fill. We haven't filled any cells in the 1lb column yet, so we can't get any more value than the laptop alone. The stereo is better than the laptop alone, so we'll take that. 1lb 2lb 3lb 4lb Stereo 0 0 0 3000 {S} Laptop 0 0 2000 {L} 3000 {S} Guitar Toaster Next we can do the Guitar Row . This one fits in the 1lb and 2lb slots, so we'll take it. When we get to the 3lb slot, we compare our potential value with the row above us: We know that we can fill a 3lb slot with a $2000 laptop, or we could grab the guitar to have 2lbs left over. What's the best thing we can fit in a 2lb slot? The 2lb slot in the row above us tells us that we don't have any valuable choices. So we'll take the $2000 laptop. However, the 4lb slot leaves us with 3lbs leftover, so we can add the contents of the 3lb slot from the row above: $2000 Laptop + $1500 Guitar = $3500 That's better than our other choice for this slot, the $3000 Stereo. 1lb 2lb 3lb 4lb Stereo 0 0 0 3000 {S} Laptop 0 0 2000 {L} 3000 {S} Guitar 1500 {G} 1500 {G} 2000 {L} 3500 {L,G} Toaster Finally, the toaster only beats the guitar by itself in the 2lb slot. 1lb 2lb 3lb 4lb Stereo 0 0 0 3000 {S} Laptop 0 0 2000 {L} 3000 {S} Guitar 1500 {G} 1500 {G} 2000 {L} 3500 {L,G} Toaster 1500 {G} 2000 {T,G} 2000 {L} 3500 {L,G} So our final answer is the contents of the 4lb slot in our last row! We'll take the Laptop and Guitar for a total of $3500. DP Questions and Clarifications What if we add another item, do we have to re-calculate everything? No, we only need to save the very last row. What if you add an item that's smaller than the smallest bucket? You need to redistribute the bucket sizes and re-calculate everything. Can you steal fractions of an item? Not with Dynamic Programming, but you wouldn't want to anyways. If you can take fractions, just use a Greedy Solution instead! Can you have items that depend on eachother? (e.g. buy one get one half off) No, the problem must be divided into discrete subproblems . They cannot be interdependent. Key Points Dynamic Programming is good when you're trying to optimize given a constraint Every solution will involve creating a grid . Your goal is to optimize the values in the cells . There is no one-size-fits-all formula . Instead this is a strategy you can use to think about optimization problems.","title":"Dynamic Programming"},{"location":"data_structures_and_algorithms/dynamic_programming/#dynamic-programming","text":"Dynamic Programming is used for optimization problems that involve a constraint . Dynamic Programming is a technique for solving hard problems, it's not a one-size-fits-all algorithm. It only works on problems that can be broken into discrete sub-problems .","title":"Dynamic Programming"},{"location":"data_structures_and_algorithms/dynamic_programming/#overview","text":"Every Dynamic Programming algorithm uses a grid to keep track of the results of the subproblem. Break a large problem into smaller subproblems (sort of like recursion) Store the results from each of the subproblems in the grid Build the final solution up by re-using the already-computed results of the subproblems","title":"Overview"},{"location":"data_structures_and_algorithms/dynamic_programming/#the-knapsack-problem","text":"You are a thief. You can only steal the things you can fit in your bag. You see these items that you might steal: Item Value Weight Stereo $3000 4 lbs Laptop $2000 3 lbs Guitar $1500 1 lbs Toaster $500 1 lbs You want to steal as much value as you can, but your bag only carries 4 lbs ! What should we take? So you can steal the Stereo for $3000, but that takes up all of your 4 lb bag space. You could take the Laptop for $2000, which uses 3 lbs of your bag space. That leaves us with 1 lb of leftover space . In that leftover space we could take the toaster or the guitar. It's obvious to us that we'd take the guitar, it's more valuable. This is the part that we should focus on: What question are we really answering here? In that leftover space we could take the toaster or the guitar. It's obvious to us that we'd take the guitar, it's more valuable. We can rephrase this question as What is the maximum value we can put in 1 lb of leftover space? But what if we had 2 lbs of leftover space ? We can't just multiply the 1 lb answer by 2, there is only one guitar that we can steal! So there is probably a different answer for all of the leftover space options: 1lb, 2lb, 3lb. Then, when we're deciding whether to take the next item, we have the current max for each of those bucket sizes. If we have leftover space, we have a pre-computed maximum value that we can fit in that space!","title":"The Knapsack Problem"},{"location":"data_structures_and_algorithms/dynamic_programming/#the-knapsack-dp-solution","text":"First, we'll setup the grid . Each column represents the solution to the subproblem where bag-size is that column number. Each row is a potential item to steal. We have a bag of size 4 , so our right-most column will have the value 4lbs . The smallest item weighs 1lbs , this will be the size of our smallest bucket & the increments between buckets. 1lb 2lb 3lb 4lb ... ... ... ... ... ... ... ... ... ... Now we can add the rows, one for each item: 1lb 2lb 3lb 4lb Stereo Laptop Guitar Toaster First we'll solve the stereo row: The stereo won't fit in any of the bags with weights 1 through 3, and we don't know anything better to put in there yet. It will fit in the 4lb slot though, so we'll mark that one. 1lb 2lb 3lb 4lb Stereo 0 0 0 3000 {S} Laptop Guitar Toaster Now we can solve the Laptop Row . It won't fit in the 1lb or 2lb colums. It will fit in the 3lb slot, and we haven't seen a better deal yet, so we'll mark it there. When we get to the 4lb slot, we can choose to take the previous best ( the cell directly above us, a $3000 stereo ). Or we can craft a solution with the current item, the Laptop, which is 3lbs . If we took the laptop, we'd have 1lb of free space leftover that we could fill. We haven't filled any cells in the 1lb column yet, so we can't get any more value than the laptop alone. The stereo is better than the laptop alone, so we'll take that. 1lb 2lb 3lb 4lb Stereo 0 0 0 3000 {S} Laptop 0 0 2000 {L} 3000 {S} Guitar Toaster Next we can do the Guitar Row . This one fits in the 1lb and 2lb slots, so we'll take it. When we get to the 3lb slot, we compare our potential value with the row above us: We know that we can fill a 3lb slot with a $2000 laptop, or we could grab the guitar to have 2lbs left over. What's the best thing we can fit in a 2lb slot? The 2lb slot in the row above us tells us that we don't have any valuable choices. So we'll take the $2000 laptop. However, the 4lb slot leaves us with 3lbs leftover, so we can add the contents of the 3lb slot from the row above: $2000 Laptop + $1500 Guitar = $3500 That's better than our other choice for this slot, the $3000 Stereo. 1lb 2lb 3lb 4lb Stereo 0 0 0 3000 {S} Laptop 0 0 2000 {L} 3000 {S} Guitar 1500 {G} 1500 {G} 2000 {L} 3500 {L,G} Toaster Finally, the toaster only beats the guitar by itself in the 2lb slot. 1lb 2lb 3lb 4lb Stereo 0 0 0 3000 {S} Laptop 0 0 2000 {L} 3000 {S} Guitar 1500 {G} 1500 {G} 2000 {L} 3500 {L,G} Toaster 1500 {G} 2000 {T,G} 2000 {L} 3500 {L,G} So our final answer is the contents of the 4lb slot in our last row! We'll take the Laptop and Guitar for a total of $3500.","title":"The Knapsack DP Solution"},{"location":"data_structures_and_algorithms/dynamic_programming/#dp-questions-and-clarifications","text":"","title":"DP Questions and Clarifications"},{"location":"data_structures_and_algorithms/dynamic_programming/#what-if-we-add-another-item-do-we-have-to-re-calculate-everything","text":"No, we only need to save the very last row.","title":"What if we add another item, do we have to re-calculate everything?"},{"location":"data_structures_and_algorithms/dynamic_programming/#what-if-you-add-an-item-thats-smaller-than-the-smallest-bucket","text":"You need to redistribute the bucket sizes and re-calculate everything.","title":"What if you add an item that's smaller than the smallest bucket?"},{"location":"data_structures_and_algorithms/dynamic_programming/#can-you-steal-fractions-of-an-item","text":"Not with Dynamic Programming, but you wouldn't want to anyways. If you can take fractions, just use a Greedy Solution instead!","title":"Can you steal fractions of an item?"},{"location":"data_structures_and_algorithms/dynamic_programming/#can-you-have-items-that-depend-on-eachother-eg-buy-one-get-one-half-off","text":"No, the problem must be divided into discrete subproblems . They cannot be interdependent.","title":"Can you have items that depend on eachother? (e.g. buy one get one half off)"},{"location":"data_structures_and_algorithms/dynamic_programming/#key-points","text":"Dynamic Programming is good when you're trying to optimize given a constraint Every solution will involve creating a grid . Your goal is to optimize the values in the cells . There is no one-size-fits-all formula . Instead this is a strategy you can use to think about optimization problems.","title":"Key Points"},{"location":"data_structures_and_algorithms/graphs/breadth_first_search/","text":"Breadth First Search (BFS) Topics Covered Graphs : Data structure used to map networks and relationships (Directed vs Undirected) Breadth First Search : Algorithm used on graphs, can answer questions like whats the shortest X? Topological Sort : A type of sorting algorithm that exposes dependencies between nodes BFS Usecase Examples BFS is good at finding shortest-paths : Write a checkers AI to calculate the fewest moves to victory Write a spellchecker (fewest number of edits to translate a misspelled word into a real one, e.g. Readed is one-off from Reader) Find the nearest doctor to you within your insurance network Introduction to Graphs A Graph models a set of objects (nodes) along with the connections (edges) between them. BFS Overview Breadth first search is an algorithm used to search through graph data structures. They allow you to answer two types of questions: Is there a path from A to B? What is the shortest path from A to B? Say for example that you're a mango farmer, and you're looking through facebook to find a mango seller. How would you check whether you're connected to a mango seller? The search is simple: Make a list of friends to search Go to each person on the list, check if they sell mangos But what if none of your friends are mango sellers? Then you need to check friends-of-friends . 3. Each time you add someone to the list of people to check, add all of their friends to the list too! Adding step (3) makes this Breadth First Search , and it will eventually cover the entire network. So we've answered the first question: Is there a path from A to B? But how do we answer the second question: What is the shortest path from A to B? The nice thing is that BFS already works this way! As long as we check items in the same order that they're added to the list , we'll automatically find the shortest path first. This will automatically search all first-degree connections before any second-degree connections; It will search all second-degree connections before any third-degree connections, and so on... But how do we ensure that we only check items in the same order that they're added to the list? We need a new data structure, called a Queue ... Queues Queues work the same way that you're used to in real life; although you might call them lines , as in: waiting in the lunch line Queues are similar to Stacks, with one key difference: Queues are FIFO (First in, First out) Stacks are LIFO (Last in, First out) Like Stacks, Queues are also bad at Random Access Queues support two operations: Enqueue (push to front) Dequeue (pop from back) Implementing a Graph in code To implement a Graph, we need two things: Nodes Edges (Connections between the nodes) So we need a data structure that lets us model the relationships between objects... Do we already know anything that can do this for us? Hash Tables ! In python, we can implement a graph with a dictionary (hash_table) . For each entry, the key will represent a particular node. The value will be a list of other nodes, meaning that there is an edge from this node to each node in the list. This is called an adjacency list # There are directed edges from you to alice, bob, and claire graph = {} graph[\"you\"] = [\"alice\", \"bob\", \"claire\"] # We can add second-degree connections like this graph[\"bob\"] = [\"anuj\", \"peggy\"] Note that the order within the adjacency list doesn't matter. Implementing the Breadth First Search Algorithm Remember that the implementation will work like this: Keep a Queue containing the people to check Pop a person off the queue Check whether this person is the target 4. - if this is the target, then we're done! return true - if this is not the target, add all of their `neighbors` to the `queue` Loop back to step 1 if the queue is empty, no route exists. return false Python supports Queues with the Deque (Double Ended Queue) from collections import deque def search(): # Create the list of people to search search_queue = deque() # Add the list of your neighbors to the search-queue search_queue += graph[\"you\"] # While the search queue is not empty while search_queue: person = search_queue.popleft() if person_is_seller(person): print(person + \" is a seller!\") return True else: search_queue += graph[person] But what happens when two people have the same friend? In this example, both bob and alice are friends with peggy, so we'll end up checking her twice! graph[\"bob\"] = [\"anuj\", \"peggy\"] graph[\"alice\"] = [\"peggy\"] To prevent this, we'll need to keep track of which people we've already checked . If we don't track the already-checked people, we might end up in an infinite loop. This could happen if the graph has a cyclic dependency : graph[\"you\"] = [\"peggy\"] graph[\"peggy\"] = [\"you\"] We would add you're neighbors (peggy) before checking you. We'd move on to checking peggy, first adding her neighbors (you) Jumping back to you, we'd add your neighbors (peggy) again... Accounting for the duplication prevention, we get: # Perform a Breadth-First-Search through \"graph\", # Starting from the entry at \"name\" # Returns the name of the nearest mango-seller or None if no such path exists def search(name, graph): # Create the list of people to search search_queue = deque() # Add the list of your neighbors to the search-queue if graph.get(name): search_queue += graph[name] # Keep track of which people we've already searched searched = [] # While the search queue is not empty while search_queue: person = search_queue.popleft() # Only search people we've never seen if not person in searched: if person_is_seller(person): return person else: # Add this persons neighbor's to the queue search_queue += graph[person] searched.append(person) return None Complexity of Breadth First Search O(V + E) If you search the entire network then you'll follow every edge, so we're at least O(E) where E = number_of_edges . Further, you'll need to add each node (vertex) to queue at some point. That's individually constant, but we do it V = number_of_vertices times, for O(V) . Combined, we get O(V + E) where V is the number of Vertices , E is the number of edges . Breadth First Search Recap BFS tells you if there's a path from A to B If a path exists, BFS find the shortest path A directed graph has arrows, the direction of the arrows represents the dependency direction. An undirected graph has no arrows, and the relationships go both ways Queues are FIFO (First in, First out) Stacks are LIFO (Last in, First out) You must check people in the order that they're added to the search_queue , otherwise you don't get the shortest path. You must keep a list of the people you've already checked to prevent duplicates , otherwise you risk infinite loops .","title":"Breadth First Search"},{"location":"data_structures_and_algorithms/graphs/breadth_first_search/#breadth-first-search-bfs","text":"","title":"Breadth First Search (BFS)"},{"location":"data_structures_and_algorithms/graphs/breadth_first_search/#topics-covered","text":"Graphs : Data structure used to map networks and relationships (Directed vs Undirected) Breadth First Search : Algorithm used on graphs, can answer questions like whats the shortest X? Topological Sort : A type of sorting algorithm that exposes dependencies between nodes","title":"Topics Covered"},{"location":"data_structures_and_algorithms/graphs/breadth_first_search/#bfs-usecase-examples","text":"BFS is good at finding shortest-paths : Write a checkers AI to calculate the fewest moves to victory Write a spellchecker (fewest number of edits to translate a misspelled word into a real one, e.g. Readed is one-off from Reader) Find the nearest doctor to you within your insurance network","title":"BFS Usecase Examples"},{"location":"data_structures_and_algorithms/graphs/breadth_first_search/#introduction-to-graphs","text":"A Graph models a set of objects (nodes) along with the connections (edges) between them.","title":"Introduction to Graphs"},{"location":"data_structures_and_algorithms/graphs/breadth_first_search/#bfs-overview","text":"Breadth first search is an algorithm used to search through graph data structures. They allow you to answer two types of questions: Is there a path from A to B? What is the shortest path from A to B? Say for example that you're a mango farmer, and you're looking through facebook to find a mango seller. How would you check whether you're connected to a mango seller? The search is simple: Make a list of friends to search Go to each person on the list, check if they sell mangos But what if none of your friends are mango sellers? Then you need to check friends-of-friends . 3. Each time you add someone to the list of people to check, add all of their friends to the list too! Adding step (3) makes this Breadth First Search , and it will eventually cover the entire network. So we've answered the first question: Is there a path from A to B? But how do we answer the second question: What is the shortest path from A to B? The nice thing is that BFS already works this way! As long as we check items in the same order that they're added to the list , we'll automatically find the shortest path first. This will automatically search all first-degree connections before any second-degree connections; It will search all second-degree connections before any third-degree connections, and so on... But how do we ensure that we only check items in the same order that they're added to the list? We need a new data structure, called a Queue ...","title":"BFS Overview"},{"location":"data_structures_and_algorithms/graphs/breadth_first_search/#queues","text":"Queues work the same way that you're used to in real life; although you might call them lines , as in: waiting in the lunch line Queues are similar to Stacks, with one key difference: Queues are FIFO (First in, First out) Stacks are LIFO (Last in, First out) Like Stacks, Queues are also bad at Random Access Queues support two operations: Enqueue (push to front) Dequeue (pop from back)","title":"Queues"},{"location":"data_structures_and_algorithms/graphs/breadth_first_search/#implementing-a-graph-in-code","text":"To implement a Graph, we need two things: Nodes Edges (Connections between the nodes) So we need a data structure that lets us model the relationships between objects... Do we already know anything that can do this for us? Hash Tables ! In python, we can implement a graph with a dictionary (hash_table) . For each entry, the key will represent a particular node. The value will be a list of other nodes, meaning that there is an edge from this node to each node in the list. This is called an adjacency list # There are directed edges from you to alice, bob, and claire graph = {} graph[\"you\"] = [\"alice\", \"bob\", \"claire\"] # We can add second-degree connections like this graph[\"bob\"] = [\"anuj\", \"peggy\"] Note that the order within the adjacency list doesn't matter.","title":"Implementing a Graph in code"},{"location":"data_structures_and_algorithms/graphs/breadth_first_search/#implementing-the-breadth-first-search-algorithm","text":"Remember that the implementation will work like this: Keep a Queue containing the people to check Pop a person off the queue Check whether this person is the target 4. - if this is the target, then we're done! return true - if this is not the target, add all of their `neighbors` to the `queue` Loop back to step 1 if the queue is empty, no route exists. return false Python supports Queues with the Deque (Double Ended Queue) from collections import deque def search(): # Create the list of people to search search_queue = deque() # Add the list of your neighbors to the search-queue search_queue += graph[\"you\"] # While the search queue is not empty while search_queue: person = search_queue.popleft() if person_is_seller(person): print(person + \" is a seller!\") return True else: search_queue += graph[person] But what happens when two people have the same friend? In this example, both bob and alice are friends with peggy, so we'll end up checking her twice! graph[\"bob\"] = [\"anuj\", \"peggy\"] graph[\"alice\"] = [\"peggy\"] To prevent this, we'll need to keep track of which people we've already checked . If we don't track the already-checked people, we might end up in an infinite loop. This could happen if the graph has a cyclic dependency : graph[\"you\"] = [\"peggy\"] graph[\"peggy\"] = [\"you\"] We would add you're neighbors (peggy) before checking you. We'd move on to checking peggy, first adding her neighbors (you) Jumping back to you, we'd add your neighbors (peggy) again... Accounting for the duplication prevention, we get: # Perform a Breadth-First-Search through \"graph\", # Starting from the entry at \"name\" # Returns the name of the nearest mango-seller or None if no such path exists def search(name, graph): # Create the list of people to search search_queue = deque() # Add the list of your neighbors to the search-queue if graph.get(name): search_queue += graph[name] # Keep track of which people we've already searched searched = [] # While the search queue is not empty while search_queue: person = search_queue.popleft() # Only search people we've never seen if not person in searched: if person_is_seller(person): return person else: # Add this persons neighbor's to the queue search_queue += graph[person] searched.append(person) return None","title":"Implementing the Breadth First Search Algorithm"},{"location":"data_structures_and_algorithms/graphs/breadth_first_search/#complexity-of-breadth-first-search-ov-e","text":"If you search the entire network then you'll follow every edge, so we're at least O(E) where E = number_of_edges . Further, you'll need to add each node (vertex) to queue at some point. That's individually constant, but we do it V = number_of_vertices times, for O(V) . Combined, we get O(V + E) where V is the number of Vertices , E is the number of edges .","title":"Complexity of Breadth First Search O(V + E)"},{"location":"data_structures_and_algorithms/graphs/breadth_first_search/#breadth-first-search-recap","text":"BFS tells you if there's a path from A to B If a path exists, BFS find the shortest path A directed graph has arrows, the direction of the arrows represents the dependency direction. An undirected graph has no arrows, and the relationships go both ways Queues are FIFO (First in, First out) Stacks are LIFO (Last in, First out) You must check people in the order that they're added to the search_queue , otherwise you don't get the shortest path. You must keep a list of the people you've already checked to prevent duplicates , otherwise you risk infinite loops .","title":"Breadth First Search Recap"},{"location":"data_structures_and_algorithms/graphs/dijkstra/","text":"Dijkstra's Algorithm Dijkstra's algorithm is run on weighted graphs to find the shortest path . It only works on Acyclic Graphs with No Negative Edges . Overview Keep a table of all of the nodes in the graph In the table, keep track of the 'cheapest' cost to get to each node At each step use the current cheapest node, see if this node allows us to get to its neighbors for cheaper than any other paths seen so far Node Cost Processed? Parent A 2 True Start B 3 False Start C NULL False NULL Dijkstra's Algorithm Steps Pick a current_node , this will be the cheapest node stored in our table so far. We need to update the cost for this node's neighbors. We should compare the neighbor's current total cost (in the table) to the theoretical cost they would have if we used a path from current_node ( total_cost_to_current_node + cost_of_edge_from_current_to_neighbor ) If this new cost is cheaper, update the neighbor's cost and mark it's parent as current_node Mark the current_node as processed, do not process it again. Repeat until every node in the graph is marked processed . Dijkstra's Algorithm vs Breadth First Search Breadth First Search finds the shortest path in terms of the number of edges in the path. BFS ignores the weights. Dijkstra's Algorithm finds the shortest path in terms of the total combined weight of the edges in the path. Graph Terminology A Weighted graph has a number assigned to each edge. This number represents the cost of using that edge. An Unweighted graph has no numbers on the edges. Each edge has the same cost. A Cycle is a path in a graph that allows you to start at a node , travel around , and end up where you started . The Key Idea of Dijkstra's Algorithm At each step, when you find the cheapest node, the cost currently stored for that node is the lowest that it can possibly ever be. It is mathematically impossible that you'll find a shorter path to this node later on. This is why Dijkstra's Algorithm breaks down when you have Negative-Weight edges within a cycle . In these cases, use something like Bellman-Ford instead. You might be tempted to think that you can get rid of the negative-weight problem by adding a positive offset to all edges. The problem is that, although an offset affects all edges equally, it does not affect all paths equally. Paths of different segment lengths will be changed by differing amounts. Length : 2 2 A --> B Length : *3* 3 A --> B Length : 2 1 1 A --> B --> C Length : *4* 2 2 A --> B --> C","title":"Dijkstra's Algorithm"},{"location":"data_structures_and_algorithms/graphs/dijkstra/#dijkstras-algorithm","text":"Dijkstra's algorithm is run on weighted graphs to find the shortest path . It only works on Acyclic Graphs with No Negative Edges .","title":"Dijkstra's Algorithm"},{"location":"data_structures_and_algorithms/graphs/dijkstra/#overview","text":"Keep a table of all of the nodes in the graph In the table, keep track of the 'cheapest' cost to get to each node At each step use the current cheapest node, see if this node allows us to get to its neighbors for cheaper than any other paths seen so far Node Cost Processed? Parent A 2 True Start B 3 False Start C NULL False NULL","title":"Overview"},{"location":"data_structures_and_algorithms/graphs/dijkstra/#dijkstras-algorithm-steps","text":"Pick a current_node , this will be the cheapest node stored in our table so far. We need to update the cost for this node's neighbors. We should compare the neighbor's current total cost (in the table) to the theoretical cost they would have if we used a path from current_node ( total_cost_to_current_node + cost_of_edge_from_current_to_neighbor ) If this new cost is cheaper, update the neighbor's cost and mark it's parent as current_node Mark the current_node as processed, do not process it again. Repeat until every node in the graph is marked processed .","title":"Dijkstra's Algorithm Steps"},{"location":"data_structures_and_algorithms/graphs/dijkstra/#dijkstras-algorithm-vs-breadth-first-search","text":"Breadth First Search finds the shortest path in terms of the number of edges in the path. BFS ignores the weights. Dijkstra's Algorithm finds the shortest path in terms of the total combined weight of the edges in the path.","title":"Dijkstra's Algorithm vs Breadth First Search"},{"location":"data_structures_and_algorithms/graphs/dijkstra/#graph-terminology","text":"A Weighted graph has a number assigned to each edge. This number represents the cost of using that edge. An Unweighted graph has no numbers on the edges. Each edge has the same cost. A Cycle is a path in a graph that allows you to start at a node , travel around , and end up where you started .","title":"Graph Terminology"},{"location":"data_structures_and_algorithms/graphs/dijkstra/#the-key-idea-of-dijkstras-algorithm","text":"At each step, when you find the cheapest node, the cost currently stored for that node is the lowest that it can possibly ever be. It is mathematically impossible that you'll find a shorter path to this node later on. This is why Dijkstra's Algorithm breaks down when you have Negative-Weight edges within a cycle . In these cases, use something like Bellman-Ford instead. You might be tempted to think that you can get rid of the negative-weight problem by adding a positive offset to all edges. The problem is that, although an offset affects all edges equally, it does not affect all paths equally. Paths of different segment lengths will be changed by differing amounts. Length : 2 2 A --> B Length : *3* 3 A --> B Length : 2 1 1 A --> B --> C Length : *4* 2 2 A --> B --> C","title":"The Key Idea of Dijkstra's Algorithm"},{"location":"data_structures_and_algorithms/greedy_algorithms/","text":"Greedy Algorithms The Greedy Strategy provides a very fast and simple solution to optimization problems. The concept is straightforward: At every step in the algorithm, take the current-best choice. Greedy algorithms find local optimizations, and don't always give the globally optimal best solution. Despite this they're often good enough, and they're extremely fast . Common Problems that can be Approximated with a Greedy Solution The greedy strategy can provide a good-enough answer to lots of optimization problems. They don't always provide the most optimal solution, but they can get close. Scheduling problems (fit the most events in a given timeframe) The Knapsack problem (fit the highest-value items in a bag of limited size) Set covering problems (from a group of sets, find the smallest subset that covers every item) NP-Complete problems (the optimal solution can only be found be checking every possible combination) The Set Datastructure A set contains a group of items with no duplicates. Like a list without duplicates Like a hash-table with just keys (not key-value pairs) first_set = set([\"A\", \"B\", \"C\"]) second_set = set([\"C\", \"D\", \"E\"]) A Set Intersection is the overlap between two sets # Contains [\"C\"] my_intersection = first_set & second_set A Union is the combination of all items in both sets # Contains [\"A\", \"B\", \"C\", \"D\", \"E\"] my_union = first_set + second_set A Difference removes the intersection of the sets # Contains [\"A\", \"B\"] my_difference = first_set - second_set Common Traits of NP-Complete problems The problem involves computing \"All Possible Combinations\" You're looking to create a sequence of items, but it's not obvious that the finished sequence is the correct answer. (e.g. Sorting is a sequence, but not NP-Complete. It's obvious that there is only one correct answer)","title":"Greedy Algorithms"},{"location":"data_structures_and_algorithms/greedy_algorithms/#greedy-algorithms","text":"The Greedy Strategy provides a very fast and simple solution to optimization problems. The concept is straightforward: At every step in the algorithm, take the current-best choice. Greedy algorithms find local optimizations, and don't always give the globally optimal best solution. Despite this they're often good enough, and they're extremely fast .","title":"Greedy Algorithms"},{"location":"data_structures_and_algorithms/greedy_algorithms/#common-problems-that-can-be-approximated-with-a-greedy-solution","text":"The greedy strategy can provide a good-enough answer to lots of optimization problems. They don't always provide the most optimal solution, but they can get close. Scheduling problems (fit the most events in a given timeframe) The Knapsack problem (fit the highest-value items in a bag of limited size) Set covering problems (from a group of sets, find the smallest subset that covers every item) NP-Complete problems (the optimal solution can only be found be checking every possible combination)","title":"Common Problems that can be Approximated with a Greedy Solution"},{"location":"data_structures_and_algorithms/greedy_algorithms/#the-set-datastructure","text":"A set contains a group of items with no duplicates. Like a list without duplicates Like a hash-table with just keys (not key-value pairs) first_set = set([\"A\", \"B\", \"C\"]) second_set = set([\"C\", \"D\", \"E\"]) A Set Intersection is the overlap between two sets # Contains [\"C\"] my_intersection = first_set & second_set A Union is the combination of all items in both sets # Contains [\"A\", \"B\", \"C\", \"D\", \"E\"] my_union = first_set + second_set A Difference removes the intersection of the sets # Contains [\"A\", \"B\"] my_difference = first_set - second_set","title":"The Set Datastructure"},{"location":"data_structures_and_algorithms/greedy_algorithms/#common-traits-of-np-complete-problems","text":"The problem involves computing \"All Possible Combinations\" You're looking to create a sequence of items, but it's not obvious that the finished sequence is the correct answer. (e.g. Sorting is a sequence, but not NP-Complete. It's obvious that there is only one correct answer)","title":"Common Traits of NP-Complete problems"},{"location":"data_structures_and_algorithms/hash_table/","text":"Hash Tables Overview Hash Tables allow you to do very quick lookup (random access) for unordered datasets. They go by several other names: Maps / Hash Maps (C/C++) Dictionaries (Python) Associative Arrays (Bash) Hash Functions Hash functions are mathematical operations that take strings as input, and provide numbers as output. Insert a unique string and get a unique number Output is consistently linked to input (the function gives the same output every time) Different inputs shouldnt map to the same output ( collision ) A hash table simply uses a hash function to map strings into positions of an array . Usecases Need fast lookup for unordered (non-sequential) data Preventing duplicate entries Use as a cache (remember recently used data) Model relationships between one object and another Hash Collisions We said that hash functions always map different keys to different slots . In reality, it's really hard to make a hash function where every input gives a different output. Dealing with Collisions The simplest way to deal with a collision is to replace the entry at a location with a linked list. Each overlapping entry would be contained in the list. Doing this too much can severly decrease the performance of the table, since you end up with a linked list, right? Hash Table Performance Average Worst Search O(1) O(n) Insert O(1) O(n) Delete O(1) O(n) To improve performance, we need to limit collisions: Keep a low Load Factor Use a good Hash Function (it should spread items around very evenly) Load Factor Load factor: (number_of_filled_slots / total_number_of_slots) It's generally a good idea to keep the load factor under 0.7 When you have to increase the size of the array, you should double the size . This will ammortize the resizing complexity","title":"Hash Tables"},{"location":"data_structures_and_algorithms/hash_table/#hash-tables","text":"","title":"Hash Tables"},{"location":"data_structures_and_algorithms/hash_table/#overview","text":"Hash Tables allow you to do very quick lookup (random access) for unordered datasets. They go by several other names: Maps / Hash Maps (C/C++) Dictionaries (Python) Associative Arrays (Bash)","title":"Overview"},{"location":"data_structures_and_algorithms/hash_table/#hash-functions","text":"Hash functions are mathematical operations that take strings as input, and provide numbers as output. Insert a unique string and get a unique number Output is consistently linked to input (the function gives the same output every time) Different inputs shouldnt map to the same output ( collision ) A hash table simply uses a hash function to map strings into positions of an array .","title":"Hash Functions"},{"location":"data_structures_and_algorithms/hash_table/#usecases","text":"Need fast lookup for unordered (non-sequential) data Preventing duplicate entries Use as a cache (remember recently used data) Model relationships between one object and another","title":"Usecases"},{"location":"data_structures_and_algorithms/hash_table/#hash-collisions","text":"We said that hash functions always map different keys to different slots . In reality, it's really hard to make a hash function where every input gives a different output.","title":"Hash Collisions"},{"location":"data_structures_and_algorithms/hash_table/#dealing-with-collisions","text":"The simplest way to deal with a collision is to replace the entry at a location with a linked list. Each overlapping entry would be contained in the list. Doing this too much can severly decrease the performance of the table, since you end up with a linked list, right?","title":"Dealing with Collisions"},{"location":"data_structures_and_algorithms/hash_table/#hash-table-performance","text":"Average Worst Search O(1) O(n) Insert O(1) O(n) Delete O(1) O(n) To improve performance, we need to limit collisions: Keep a low Load Factor Use a good Hash Function (it should spread items around very evenly)","title":"Hash Table Performance"},{"location":"data_structures_and_algorithms/hash_table/#load-factor","text":"Load factor: (number_of_filled_slots / total_number_of_slots) It's generally a good idea to keep the load factor under 0.7 When you have to increase the size of the array, you should double the size . This will ammortize the resizing complexity","title":"Load Factor"},{"location":"data_structures_and_algorithms/linked_list/","text":"Linked Lists Overview A Linked List is a set of elements where each node stores the address of the next node. Advantages of Linked Lists over Arrays Imagine you're at lunch with a group of four friends. You'd like to sit together at one table, so you take a small table with four spots. You sit down and start enjoying your meal, when a fifth friend shows up. Now, everybody has to stand up as a group and look for a bigger table. This is the same problem we see with adding elements to contiguous arrays. We have to store all of the elements in the same area of memory. If we need more memory, we have to move ALL of the elements to a new segment. This can be very expensive! This is called random insertion , which Linked Lists are really good at! Insertion Complexity Linked List: O(1) Array: O(n) Disadvantages of Linked Lists over Arrays Have you ever visited one of those clickbait \"Top Ten Bands of All Time\" websites, where each item is a separate webpage? They start you all the way at number 10 so you have to click the next button over and over to who the number one pick is. You'd really rather jump right to the front and get it over with. This is called random access , and Linked Lists are really bad at it. Every time you want to access an item, you have to start at the head and work your way down. Reading Complexity Linked List: O(n) Array: O(1) Summary Linked Lists are useful if you're: Doing lots of inserts Don't need to do random read often Are planning to read every element anyway","title":"Linked Lists"},{"location":"data_structures_and_algorithms/linked_list/#linked-lists","text":"","title":"Linked Lists"},{"location":"data_structures_and_algorithms/linked_list/#overview","text":"A Linked List is a set of elements where each node stores the address of the next node.","title":"Overview"},{"location":"data_structures_and_algorithms/linked_list/#advantages-of-linked-lists-over-arrays","text":"Imagine you're at lunch with a group of four friends. You'd like to sit together at one table, so you take a small table with four spots. You sit down and start enjoying your meal, when a fifth friend shows up. Now, everybody has to stand up as a group and look for a bigger table. This is the same problem we see with adding elements to contiguous arrays. We have to store all of the elements in the same area of memory. If we need more memory, we have to move ALL of the elements to a new segment. This can be very expensive! This is called random insertion , which Linked Lists are really good at!","title":"Advantages of Linked Lists over Arrays"},{"location":"data_structures_and_algorithms/linked_list/#insertion-complexity","text":"Linked List: O(1) Array: O(n)","title":"Insertion Complexity"},{"location":"data_structures_and_algorithms/linked_list/#disadvantages-of-linked-lists-over-arrays","text":"Have you ever visited one of those clickbait \"Top Ten Bands of All Time\" websites, where each item is a separate webpage? They start you all the way at number 10 so you have to click the next button over and over to who the number one pick is. You'd really rather jump right to the front and get it over with. This is called random access , and Linked Lists are really bad at it. Every time you want to access an item, you have to start at the head and work your way down.","title":"Disadvantages of Linked Lists over Arrays"},{"location":"data_structures_and_algorithms/linked_list/#reading-complexity","text":"Linked List: O(n) Array: O(1)","title":"Reading Complexity"},{"location":"data_structures_and_algorithms/linked_list/#summary","text":"Linked Lists are useful if you're: Doing lots of inserts Don't need to do random read often Are planning to read every element anyway","title":"Summary"},{"location":"data_structures_and_algorithms/linked_list/challenges/cycle_check/","text":"Challenge: Linked List Cycle Test Given the head of a linked list, determine if the list contains a cycle. A cycle means there's a way to leave from a point, and return to that same point later. Return true if there's a cycle, false otherwise. Problem Solving A simple solution is to keep track of every node that we've seen. We could add each node to a hash-table as we pass through. Then, every time we hit a node, we could see if we have already stored it in the hash-table. However, this solution uses O(n) extra memory. Is there a way to do it in O(1) memory? If we send out two pointers that move at different speeds , then they will eventually catch up to one-another. One pointer is a leader , and another is the follower . The leader will move faster than the follower. If we hit a nullptr , we're at the end and there's no cycle. If we see that leader and follower are the same, we must have hit a cycle . Part 2: Return the node where the cycle begins We're going to creep the head node forward, and try to detect it with the other pointers that we know are already in the cycle. We can send the leader around the cycle repeatedly, watching for follower . When we hit follower , we move head forward.","title":"Challenge: Linked List Cycle Test"},{"location":"data_structures_and_algorithms/linked_list/challenges/cycle_check/#challenge-linked-list-cycle-test","text":"Given the head of a linked list, determine if the list contains a cycle. A cycle means there's a way to leave from a point, and return to that same point later. Return true if there's a cycle, false otherwise.","title":"Challenge: Linked List Cycle Test"},{"location":"data_structures_and_algorithms/linked_list/challenges/cycle_check/#problem-solving","text":"A simple solution is to keep track of every node that we've seen. We could add each node to a hash-table as we pass through. Then, every time we hit a node, we could see if we have already stored it in the hash-table. However, this solution uses O(n) extra memory. Is there a way to do it in O(1) memory? If we send out two pointers that move at different speeds , then they will eventually catch up to one-another. One pointer is a leader , and another is the follower . The leader will move faster than the follower. If we hit a nullptr , we're at the end and there's no cycle. If we see that leader and follower are the same, we must have hit a cycle .","title":"Problem Solving"},{"location":"data_structures_and_algorithms/linked_list/challenges/cycle_check/#part-2-return-the-node-where-the-cycle-begins","text":"We're going to creep the head node forward, and try to detect it with the other pointers that we know are already in the cycle. We can send the leader around the cycle repeatedly, watching for follower . When we hit follower , we move head forward.","title":"Part 2: Return the node where the cycle begins"},{"location":"data_structures_and_algorithms/linked_list/challenges/design_a_linked_list/design_linked_list/","text":"Challenge: Design a Linked List Design a singly linked list such that each node has a val and next attribute. Implement the MyLinkedList class: int get(int index) get the value of the node at index. If the index is invalid, return -1 void addAtHead(int val) add a node with value val before the first element. The new node becomes the head of the list. void addAtTail(int val) append a node with value val at the end of the list. void addAtIndex(int index, int val) insert a node with value val at the specified index . If index equals the length of the list, append to the end. If the index is invalid, do not insert the node. void deleteAtIndex(int index) delete the node at the specified index . If index is invalid, do nothing. Constraints: 0 <= index, val <= 1000 at most 200 calls \"MyLinkedList\", 7 \"addAtHead\", 7 \"addAtTail\", 9 \"addAtHead\", 8 \"addAtTail\", 6 \"addAtHead\", 0 \"addAtHead\", 5 \"get\", => 8 0 \"addAtHead\", 2 \"get\", => 6 5 \"get\", => 7 4 \"addAtTail\"","title":"Challenge: Design a Linked List"},{"location":"data_structures_and_algorithms/linked_list/challenges/design_a_linked_list/design_linked_list/#challenge-design-a-linked-list","text":"Design a singly linked list such that each node has a val and next attribute. Implement the MyLinkedList class: int get(int index) get the value of the node at index. If the index is invalid, return -1 void addAtHead(int val) add a node with value val before the first element. The new node becomes the head of the list. void addAtTail(int val) append a node with value val at the end of the list. void addAtIndex(int index, int val) insert a node with value val at the specified index . If index equals the length of the list, append to the end. If the index is invalid, do not insert the node. void deleteAtIndex(int index) delete the node at the specified index . If index is invalid, do nothing.","title":"Challenge: Design a Linked List"},{"location":"data_structures_and_algorithms/linked_list/challenges/design_a_linked_list/design_linked_list/#constraints","text":"0 <= index, val <= 1000 at most 200 calls \"MyLinkedList\", 7 \"addAtHead\", 7 \"addAtTail\", 9 \"addAtHead\", 8 \"addAtTail\", 6 \"addAtHead\", 0 \"addAtHead\", 5 \"get\", => 8 0 \"addAtHead\", 2 \"get\", => 6 5 \"get\", => 7 4 \"addAtTail\"","title":"Constraints:"},{"location":"data_structures_and_algorithms/recursion/","text":"Recursion Overview Imagine that you find yourself crouching around the dusty attic at your grandmas, when you find an old locked chest. You ask where the key is, and she points to a cardboard box in the corner. You open the box to find a set of smaller boxes. If you want to find the key in this nested set of boxes, you might use a recursive algorithm. Look through the box If you find a box, open it and go back to step 1 If you find a key, youre done! Loops can perform better, but recursion can be more readable in some cases (or the only option!) Base Case The Base Case tells you when youre finished and can stop recursing Recursive Case The Recursive Case is the part that you keep jumping back into until you end up with the base case The Stack Datastructure A Stack allows you to do two actions: Push a new item to the top Pop the top item off the stack Notes Recursion is when functions call themselves Every recursive function has a base case and a recursive case A Stack can push and pop Functions are added on top of the callstack , which can get very large, take a lot of memory, and overflow When working with recursive array functions , the base case is often an empty array or one-element array","title":"Recursion"},{"location":"data_structures_and_algorithms/recursion/#recursion","text":"","title":"Recursion"},{"location":"data_structures_and_algorithms/recursion/#overview","text":"Imagine that you find yourself crouching around the dusty attic at your grandmas, when you find an old locked chest. You ask where the key is, and she points to a cardboard box in the corner. You open the box to find a set of smaller boxes. If you want to find the key in this nested set of boxes, you might use a recursive algorithm. Look through the box If you find a box, open it and go back to step 1 If you find a key, youre done! Loops can perform better, but recursion can be more readable in some cases (or the only option!)","title":"Overview"},{"location":"data_structures_and_algorithms/recursion/#base-case","text":"The Base Case tells you when youre finished and can stop recursing","title":"Base Case"},{"location":"data_structures_and_algorithms/recursion/#recursive-case","text":"The Recursive Case is the part that you keep jumping back into until you end up with the base case","title":"Recursive Case"},{"location":"data_structures_and_algorithms/recursion/#the-stack-datastructure","text":"A Stack allows you to do two actions: Push a new item to the top Pop the top item off the stack","title":"The Stack Datastructure"},{"location":"data_structures_and_algorithms/recursion/#notes","text":"Recursion is when functions call themselves Every recursive function has a base case and a recursive case A Stack can push and pop Functions are added on top of the callstack , which can get very large, take a lot of memory, and overflow When working with recursive array functions , the base case is often an empty array or one-element array","title":"Notes"},{"location":"data_structures_and_algorithms/sorting/quicksort/","text":"Quicksort Overview Quicksort uses Divide and Conquer to recursively sort an array of elements Pick a pivot , put smaller elements on the left, larger elements on the right (this is called partitioning ) Recursively sort the two smaller sub-arrays Average case O(n log n) , worst case O(n^2) Base Cases These arrays are really easy to sort: [] : Empty Array, already sorted [5] : Single Element, already sorted Two elements are a tiny bit harder, but still easy: [3, 7] : left < right, already sorted [7, 3] : left > right, just swap the elements Recursive Cases Pick an element to be the Pivot Move elements less than the pivot to the left sub-array Move elements greater than the pivot to the right sub-array Recursively call quicksort on the sub-arrays, return to step (1) Quicksort Complexity Quicksort is a little strange, the complexity depends on which pivot we choose. We get the best performance when we pick the pivot completely randomly every time. Average Case: O(n logn) Worst Case: O(n^2) Notes Quicksort is actually faster than Merge Sort , even though they have the same complexity O(n logn)","title":"Quick Sort"},{"location":"data_structures_and_algorithms/sorting/quicksort/#quicksort","text":"","title":"Quicksort"},{"location":"data_structures_and_algorithms/sorting/quicksort/#overview","text":"Quicksort uses Divide and Conquer to recursively sort an array of elements Pick a pivot , put smaller elements on the left, larger elements on the right (this is called partitioning ) Recursively sort the two smaller sub-arrays Average case O(n log n) , worst case O(n^2)","title":"Overview"},{"location":"data_structures_and_algorithms/sorting/quicksort/#base-cases","text":"These arrays are really easy to sort: [] : Empty Array, already sorted [5] : Single Element, already sorted Two elements are a tiny bit harder, but still easy: [3, 7] : left < right, already sorted [7, 3] : left > right, just swap the elements","title":"Base Cases"},{"location":"data_structures_and_algorithms/sorting/quicksort/#recursive-cases","text":"Pick an element to be the Pivot Move elements less than the pivot to the left sub-array Move elements greater than the pivot to the right sub-array Recursively call quicksort on the sub-arrays, return to step (1)","title":"Recursive Cases"},{"location":"data_structures_and_algorithms/sorting/quicksort/#quicksort-complexity","text":"Quicksort is a little strange, the complexity depends on which pivot we choose. We get the best performance when we pick the pivot completely randomly every time. Average Case: O(n logn) Worst Case: O(n^2)","title":"Quicksort Complexity"},{"location":"data_structures_and_algorithms/sorting/quicksort/#notes","text":"Quicksort is actually faster than Merge Sort , even though they have the same complexity O(n logn)","title":"Notes"},{"location":"data_structures_and_algorithms/sorting/selection_sort/","text":"Selection Sort Overview Selection sort is a slow sorting algorithm. The action is: Iterate over the whole list, find the largest value Move that largest value to the top Repeat this process for each position Complexity Selection sort has a Quadratic O(n^2) time complexity","title":"Selection Sort"},{"location":"data_structures_and_algorithms/sorting/selection_sort/#selection-sort","text":"","title":"Selection Sort"},{"location":"data_structures_and_algorithms/sorting/selection_sort/#overview","text":"Selection sort is a slow sorting algorithm. The action is: Iterate over the whole list, find the largest value Move that largest value to the top Repeat this process for each position","title":"Overview"},{"location":"data_structures_and_algorithms/sorting/selection_sort/#complexity","text":"Selection sort has a Quadratic O(n^2) time complexity","title":"Complexity"},{"location":"docker/","text":"Docker This is my set of personal notes on the Docker paradigm, collected while completing the Katacode Docker Courses . Although I've restructured and restated the information to my preferences, some information is directly duplicated. Please check out the Katacode courses, they're an excellent entrypoint into the ecosystem. Table of Contents Docker Overview Building Images Stateful Data Containers Container Networking Docker Compose Docker Clusters Quick Reference # Build the image defined in the current directory docker build -t <name> . # Run a container docker run <name> docker run -d <name> # as a \"detatched\" background process docker run -it <name> # as an \"interactive terminal\" foreground process docker run -e NODE_ENV=production # set the NODE_ENV environment variable # Start services defined in docker-compose.yml docker-compose up # List all running containers docker ps docker ps -q # quiet, only show the container id's # Get more details about a container docker inspect <name|id> # Display messages the container has written to stderr or stdout docker logs <name|id> # Find existing images on the Docker registry docker search <image>","title":"Introduction"},{"location":"docker/#docker","text":"This is my set of personal notes on the Docker paradigm, collected while completing the Katacode Docker Courses . Although I've restructured and restated the information to my preferences, some information is directly duplicated. Please check out the Katacode courses, they're an excellent entrypoint into the ecosystem.","title":"Docker"},{"location":"docker/#table-of-contents","text":"Docker Overview Building Images Stateful Data Containers Container Networking Docker Compose Docker Clusters","title":"Table of Contents"},{"location":"docker/#quick-reference","text":"# Build the image defined in the current directory docker build -t <name> . # Run a container docker run <name> docker run -d <name> # as a \"detatched\" background process docker run -it <name> # as an \"interactive terminal\" foreground process docker run -e NODE_ENV=production # set the NODE_ENV environment variable # Start services defined in docker-compose.yml docker-compose up # List all running containers docker ps docker ps -q # quiet, only show the container id's # Get more details about a container docker inspect <name|id> # Display messages the container has written to stderr or stdout docker logs <name|id> # Find existing images on the Docker registry docker search <image>","title":"Quick Reference"},{"location":"docker/Building_Docker_Images/","text":"Docker Images Docker images start from a Base Image . The base image should include all dependencies required by your application. The base image is defined as an instruction in the Dockerfile . The Dockerfile contains instructions on how to build the docker image . # Define the base image FROM nginx:alpine # Copy the content of the current directory to a specified location within the container COPY . /usr/share/nginx/html Now you can build and run the dockerfile. #Adding the `-t` flag allows us to specify a user-friendly image name. docker build -t webserver-image:v1 . docker run -d -p 80:80 webserver-image:v1 Building Images We use a Base Image as the foundation of other images. # `FROM` is used to specify the base image: FROM <image-name>:<tag> FROM nginx:1.11-alpine # `RUN` allows you to execute any command like you would from the terminal. # Results are persistent within the image! # `COPY <src> <dst>` allows you to copy file into the container's image COPY index.html /usr/share/nginx/html/index.html # `EXPOSE <port>` indicates which ports should be open and bound. EXPOSE 80 # `CMD` Defines the default command to be run when the container is launched CMD [\"nginx\", \"-a\", \"arga val\", \"-b\", \"argb val\"] # `ENTRYPOINT` is an alternative to CMD. # CMD is completely overwritten if arguments are provided. # With ENTRYPOINT, arguments are instead appended to the command. OnBuild Optimization We can include the ONBUILD keyword to indicate that a command should be run later, when the image is used as a Base Build for some other image. FROM node:7 RUN mkdir -p /usr/src/app ONBUILD COPY package.json /usr/src/app ONBUILD RUN npm install ONBUILD COPY . /usr/src/app CMD [\"npm\", \"start\"] This way, the Dockerfile for our real application can be much simpler, we just have to set the port! From node:7-onbuild EXPOSE 3000 Ignoring files during build Say for example you have a sensitive file \"passwords.txt\", but we need it during the build. One option is to COPY -> use -> DELETE . You can also ignore files with a .dockerignore file, it works just like a .gitignore . You'll want to ignore \".git\" directories, along with dependencies that are downloaded/built like node modules. Optimizing Images with Multi-Stage Builds We'll use multi-stage builds to make our images smaller, and faster to build. Previously, this problem would have been solved with two dockerfiles: The first would have the steps to build the binary and artifacts using a development container. The second would be optimized for production, it would not include development tools Multi Stage Dockerfile # First Stage # Use the Golang SDK to build a binary FROM golang:1.6-alpine RUN mkdir /app ADD . /app/ WORKDIR /app RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o main . # Second Stage # Copy the resulting binary into an optimized Docker Image FROM alpine EXPOSE 80 CMD [\"/app\"] COPY --from=0 /app/main /app We'll build the image with the same syntax as before: docker build -f Dockerfile.multi -t golang-app . This will result in two images. An untagged image that was used during the first stage Our smaller target image We can test the image with no other changes required. docker run -d -p 80:80 golang-app curl localhost up","title":"Building Docker Images"},{"location":"docker/Building_Docker_Images/#docker-images","text":"Docker images start from a Base Image . The base image should include all dependencies required by your application. The base image is defined as an instruction in the Dockerfile . The Dockerfile contains instructions on how to build the docker image . # Define the base image FROM nginx:alpine # Copy the content of the current directory to a specified location within the container COPY . /usr/share/nginx/html Now you can build and run the dockerfile. #Adding the `-t` flag allows us to specify a user-friendly image name. docker build -t webserver-image:v1 . docker run -d -p 80:80 webserver-image:v1","title":"Docker Images"},{"location":"docker/Building_Docker_Images/#building-images","text":"We use a Base Image as the foundation of other images. # `FROM` is used to specify the base image: FROM <image-name>:<tag> FROM nginx:1.11-alpine # `RUN` allows you to execute any command like you would from the terminal. # Results are persistent within the image! # `COPY <src> <dst>` allows you to copy file into the container's image COPY index.html /usr/share/nginx/html/index.html # `EXPOSE <port>` indicates which ports should be open and bound. EXPOSE 80 # `CMD` Defines the default command to be run when the container is launched CMD [\"nginx\", \"-a\", \"arga val\", \"-b\", \"argb val\"] # `ENTRYPOINT` is an alternative to CMD. # CMD is completely overwritten if arguments are provided. # With ENTRYPOINT, arguments are instead appended to the command.","title":"Building Images"},{"location":"docker/Building_Docker_Images/#onbuild-optimization","text":"We can include the ONBUILD keyword to indicate that a command should be run later, when the image is used as a Base Build for some other image. FROM node:7 RUN mkdir -p /usr/src/app ONBUILD COPY package.json /usr/src/app ONBUILD RUN npm install ONBUILD COPY . /usr/src/app CMD [\"npm\", \"start\"] This way, the Dockerfile for our real application can be much simpler, we just have to set the port! From node:7-onbuild EXPOSE 3000","title":"OnBuild Optimization"},{"location":"docker/Building_Docker_Images/#ignoring-files-during-build","text":"Say for example you have a sensitive file \"passwords.txt\", but we need it during the build. One option is to COPY -> use -> DELETE . You can also ignore files with a .dockerignore file, it works just like a .gitignore . You'll want to ignore \".git\" directories, along with dependencies that are downloaded/built like node modules.","title":"Ignoring files during build"},{"location":"docker/Building_Docker_Images/#optimizing-images-with-multi-stage-builds","text":"We'll use multi-stage builds to make our images smaller, and faster to build. Previously, this problem would have been solved with two dockerfiles: The first would have the steps to build the binary and artifacts using a development container. The second would be optimized for production, it would not include development tools","title":"Optimizing Images with Multi-Stage Builds"},{"location":"docker/Building_Docker_Images/#multi-stage-dockerfile","text":"# First Stage # Use the Golang SDK to build a binary FROM golang:1.6-alpine RUN mkdir /app ADD . /app/ WORKDIR /app RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o main . # Second Stage # Copy the resulting binary into an optimized Docker Image FROM alpine EXPOSE 80 CMD [\"/app\"] COPY --from=0 /app/main /app We'll build the image with the same syntax as before: docker build -f Dockerfile.multi -t golang-app . This will result in two images. An untagged image that was used during the first stage Our smaller target image We can test the image with no other changes required. docker run -d -p 80:80 golang-app curl localhost up","title":"Multi Stage Dockerfile"},{"location":"docker/Container_Networking/","text":"Communicating Between Containers In this example, we'll use Redis , which is a fast, open-source, key-value data store. docker run -d --name redis-server redis Create a link When we're launching a new container, we can connect to a source container with the --link <container-name|id>:<alias> . Here, we'll link an Alpine server to our redis-server host. First, Docker will set some environment variables based on the linked container. These give you a way to reference information like Ports, IP addresses via known names. You can output the environment variables with the env command: docker run --link redis-server:redis alpine env Second, Docker will update the HOSTS file of the container. It will add an entry for our source container consisting of three names: the original, the alias, and the hash-id. You can see the entry in the hosts file at /etc/hosts : docker run --link redis-server:redis alpine cat /etc/hosts This is the full command: We'll run an alpine container, link it to our redis server, and ping redis to make sure it's connected. docker run --link redis-server:redis alpine ping -c 1 redis Connect to an App Using this link method, we can connect applications in the usual way, as if they were not running in containers. docker run -d -p 3000:3000 --link redis-server:redis katacoda/redis-node-docker-example We'll test the connection with curl curl docker:3000 Networks between Containers Instead of using links , we'll use networks to allow containers to come and go more freely. Docker has an Embedded DNS Server. Create a network # Create a network docker network create backend-network # Connect to a network docker run -d --name=redis --net=backend-network redis # Get some info about the network docker run --net=backend-network alpine env docker run --net=backend-network alpine cat /etc/hosts # Check that the DNS server is assigned to the container in resolv.conf # It should be 127.0.0.11 in this case docker run --net=backend-network alpine cat /etc/resolv.conf # Check that the DNS server returns the IP address of the correct Container docker run --net=backend-network alpine ping -c1 redis Connect two containers # Create a network docker network create frontend-network # Attach existing containers to the network docker network connect frontend-network redis # Launch the webserver, connect it to our network docker run -d -p 3000:3000 --net=frontend-network katacoda/redis-node-docker-example # Test with curl curl docker:3000 Create Aliases Links still work when using networks. They provide an alias for a container name. This gives the container an extra DNS entry name and way to be discovered. # We'll create another network docker network create frontend-network2 # Connect our Redis instance to the network with the alias \"db\" docker network connect --alias db frontend-network2 redis # We'll run an instance of alpine to ping the db docker run --net=frontend-network2 alpine ping -c1 db Disconnect Containers # List all the networks on our host docker network ls # Explore the network, see which containers are attached. docker network inspect frontend-network # Disconnect our redis container from the frontend-network docker network disconnect frontend-network redis up","title":"Container Networking"},{"location":"docker/Container_Networking/#communicating-between-containers","text":"In this example, we'll use Redis , which is a fast, open-source, key-value data store. docker run -d --name redis-server redis","title":"Communicating Between Containers"},{"location":"docker/Container_Networking/#create-a-link","text":"When we're launching a new container, we can connect to a source container with the --link <container-name|id>:<alias> . Here, we'll link an Alpine server to our redis-server host. First, Docker will set some environment variables based on the linked container. These give you a way to reference information like Ports, IP addresses via known names. You can output the environment variables with the env command: docker run --link redis-server:redis alpine env Second, Docker will update the HOSTS file of the container. It will add an entry for our source container consisting of three names: the original, the alias, and the hash-id. You can see the entry in the hosts file at /etc/hosts : docker run --link redis-server:redis alpine cat /etc/hosts This is the full command: We'll run an alpine container, link it to our redis server, and ping redis to make sure it's connected. docker run --link redis-server:redis alpine ping -c 1 redis","title":"Create a link"},{"location":"docker/Container_Networking/#connect-to-an-app","text":"Using this link method, we can connect applications in the usual way, as if they were not running in containers. docker run -d -p 3000:3000 --link redis-server:redis katacoda/redis-node-docker-example We'll test the connection with curl curl docker:3000","title":"Connect to an App"},{"location":"docker/Container_Networking/#networks-between-containers","text":"Instead of using links , we'll use networks to allow containers to come and go more freely. Docker has an Embedded DNS Server.","title":"Networks between Containers"},{"location":"docker/Container_Networking/#create-a-network","text":"# Create a network docker network create backend-network # Connect to a network docker run -d --name=redis --net=backend-network redis # Get some info about the network docker run --net=backend-network alpine env docker run --net=backend-network alpine cat /etc/hosts # Check that the DNS server is assigned to the container in resolv.conf # It should be 127.0.0.11 in this case docker run --net=backend-network alpine cat /etc/resolv.conf # Check that the DNS server returns the IP address of the correct Container docker run --net=backend-network alpine ping -c1 redis","title":"Create a network"},{"location":"docker/Container_Networking/#connect-two-containers","text":"# Create a network docker network create frontend-network # Attach existing containers to the network docker network connect frontend-network redis # Launch the webserver, connect it to our network docker run -d -p 3000:3000 --net=frontend-network katacoda/redis-node-docker-example # Test with curl curl docker:3000","title":"Connect two containers"},{"location":"docker/Container_Networking/#create-aliases","text":"Links still work when using networks. They provide an alias for a container name. This gives the container an extra DNS entry name and way to be discovered. # We'll create another network docker network create frontend-network2 # Connect our Redis instance to the network with the alias \"db\" docker network connect --alias db frontend-network2 redis # We'll run an instance of alpine to ping the db docker run --net=frontend-network2 alpine ping -c1 db","title":"Create Aliases"},{"location":"docker/Container_Networking/#disconnect-containers","text":"# List all the networks on our host docker network ls # Explore the network, see which containers are attached. docker network inspect frontend-network # Disconnect our redis container from the frontend-network docker network disconnect frontend-network redis up","title":"Disconnect Containers"},{"location":"docker/Docker_Clusters/","text":"Load Balancing Containers We'll explore how to use the NGINX web server to load balance requests between two containers. Docker provides serveral ways for containers to communicate. We explored links, which configure the container using environment variables and host entry's. The Service Discovery Pattern is where the application uses a third-party system to identify the location of a target service. For example, if our application wanted to talk to a database, it would ask an API what the address of the database is. This pattern allows you to quickly reconfigure and scale your architecture with better fault tolerance than fixed locations. NGINX Proxy In this example, we want an NGINX service which can dynamically discover and update its load balancing configuration when new containers are added. This implementation already exists, it's called nginx-proxy . Nginx-proxy accepts HTTP requests, then proxies the request to the appropriate container (based on the request Hostname). There are three key properties that must be configured when launching a proxy container: The container must be bound to port 80 on the host. This ensures that all HTTP requests are handled by the proxy. The docker.sock file must be mounted. It's a connection to the docker daemon running on the host Nginx-proxy uses this to listen for events We can set an optional -e DEFAULTHOST=<domain> If a request arrives without a specified host, it will be redirected here. docker run -d -p 80:80 -e DEFAULT_HOST=proxy.example -v /var/run/docker.sock:/tmp/docker.sock:ro --name nginx jwilder/nginx-proxy # Because we've added DEFAULT_HOST, # any requests that arrive will be directed to the container # that has been assigned with the host \"proxy.example\" # We'll make a request, but we expect a 503 error since we have no containers curl http://docker Single Host Now Nginx-proxy is listening to events from Docker. For Nginx-proxy to start sending requests to a container, we need to set the environment variable VIRTUAL_HOST . This defines where requests will come from. In this case, we'll match the virtual host to the default host, proxy.example docker run -d -p 80 -e VIRTUAL_HOST=proxy.example katacoda/docker-http-server # Now our request should be processed curl http://docker Cluster If we were to launch a secon container with the same VIRTUAL_HOST, then nginx-proxy will configure the system in a round-robin load balancing scenario: The first request is sent to the first container The second request is sent to the second container # Launch a second container using the same command as before docker run -d -p 80 -e VIRTUAL_HOST=proxy.example katacoda/docker-http-server # If we repeatedly send requests, we'll see them being alternativly handled by the two containers curl http://docker Generated NGINX Configuration While nginx-proxy will automatically create and configure NGINX for us, we can see the final configuration that it made: docker exec nginx cat /etc/nginx/conf.d/default.conf Docker Swarm We'll learn how to initialize a cluster in Docker Swarm Mode and deploy networked containers using the built-in Docker Orchestration . Swarm Mode enables the ability to deploy containers across multiple Docker hosts . It uses overlay networks for service discovery and a built-in load balancer for scaling the services. Key Concepts Node : A Node is an instance of the Docker Engine connected to the swarm. Nodes are either managers or workers . Managers schedule which containers to run and where Workers execute the tasks Services : A Service is a high-level concept relating to a collection of tasks to be executed by workers. An example of a service is an HTTP server running as a Docker Container on three nodes. Load Balancing : Docker includes a built-in load balancer to process requests across all containers. Initialize Swarm Mode By default, Docker works as an isolated single-node. Swarm Mode turns it into a multi-host cluster-aware engine. The first node to initialize the Swarm Mode becomes the Manager . As new nodes join, they can adjust their roles between managers or workers. You should run 3 to 5 managers in production to ensure high availablility # You can find an overview of the commands with --help $docker swarm --help # Create a Swarm Mode Cluster $docker swarm init Swarm initialized: current node (dy72cua9m3ruay1ebxelb1un2) is now a manager. To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-53u343zuq4nvb66b2qj62deyddj4q03ozj5npqu8fp36ag8tji-adckignv2e49vcx4vkjr8jjjr 172.17.0.41:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. Join the Cluster If some nodes crash, the containers which were running on those hosts will be automatically rescheduled onto other available nodes, ensuring high-availability. Docker uses an additional port 2377 for managing the Swarm. This port should be blocked from public access, and only accessed by trusted users and nodes. Let's get the token that's required to add a worker to the cluster. In this demo, we'll get it with swarm join-token , but in production, this token should be stored securely and only given to trusted individuals . # Ask the manager what the token is token=$(ssh -o StrictHostKeyChecking=no 172.17.0.41 \"docker swarm join-token -q worker\") && echo $token # On the second host, join the cluster docker swarm join 172.17.0.41:2377 --token $token # Now we can view all nodes in the cluster docker node ls Create an Overlay Network In previous versions, Docker required an external key-value store (like Consul) to ensure consistency across the network. Now this functionality is incorporated internally into docker. The syntax is the same as before, but we add the overlay keyword when creating the network. docker network create -d overlay skynet Deploy a Service By default, docker uses a Spread Replication Model to decide which containers should run on which hosts. The spread approach ensures that containers are deployed evenly across the cluster . If a node is removed from the cluster, the workload is rescheduled across the remaining nodes . In this example, we're deploying the image katacoda/docker-http-server with the name http . It will be attached to the skynet network that we just created. docker service create --name http --network skynet For replication and availability, we'll run two instances of replicas of the container across the cluster. --replicas 2 Finally, we load balance the containers together on port 80. If we send an HTTP request to any of the nodes, the cluster will decide which node to respond on. The node which accepted the request may be different from the node where the container responds. In full: # Create the \"http\" service on the \"skynet\" network docker service create --name http --network skynet --replicas 2 -p 80:80 katacoda/docker-http-server # View the currently-running services docker service ls # If we make repeated HTTP requests, we'll see alternate hosts handling the requests curl host01 Inspect State We can use the Service concept to inspect the health and state of the cluster. # View the list of all tasks associated with the \"http\" service docker service ps http # View the details of a particular service docker service inspect --pretty http # On each node, we can we what task we're currently running # \"self\" refers to the manager node Leader docker node ps self # We can use the ID of a node to query individual hosts docker node ps $(docker node ls -q | head -n1) Scale the Service A Service allows us to scale the number of instances of a task that are running across the cluster. # Run the http service across 5 containers docker service scale http=5 # We can see the new nodes on each host docker ps up","title":"Docker Clusters"},{"location":"docker/Docker_Clusters/#load-balancing-containers","text":"We'll explore how to use the NGINX web server to load balance requests between two containers. Docker provides serveral ways for containers to communicate. We explored links, which configure the container using environment variables and host entry's. The Service Discovery Pattern is where the application uses a third-party system to identify the location of a target service. For example, if our application wanted to talk to a database, it would ask an API what the address of the database is. This pattern allows you to quickly reconfigure and scale your architecture with better fault tolerance than fixed locations.","title":"Load Balancing Containers"},{"location":"docker/Docker_Clusters/#nginx-proxy","text":"In this example, we want an NGINX service which can dynamically discover and update its load balancing configuration when new containers are added. This implementation already exists, it's called nginx-proxy . Nginx-proxy accepts HTTP requests, then proxies the request to the appropriate container (based on the request Hostname). There are three key properties that must be configured when launching a proxy container: The container must be bound to port 80 on the host. This ensures that all HTTP requests are handled by the proxy. The docker.sock file must be mounted. It's a connection to the docker daemon running on the host Nginx-proxy uses this to listen for events We can set an optional -e DEFAULTHOST=<domain> If a request arrives without a specified host, it will be redirected here. docker run -d -p 80:80 -e DEFAULT_HOST=proxy.example -v /var/run/docker.sock:/tmp/docker.sock:ro --name nginx jwilder/nginx-proxy # Because we've added DEFAULT_HOST, # any requests that arrive will be directed to the container # that has been assigned with the host \"proxy.example\" # We'll make a request, but we expect a 503 error since we have no containers curl http://docker","title":"NGINX Proxy"},{"location":"docker/Docker_Clusters/#single-host","text":"Now Nginx-proxy is listening to events from Docker. For Nginx-proxy to start sending requests to a container, we need to set the environment variable VIRTUAL_HOST . This defines where requests will come from. In this case, we'll match the virtual host to the default host, proxy.example docker run -d -p 80 -e VIRTUAL_HOST=proxy.example katacoda/docker-http-server # Now our request should be processed curl http://docker","title":"Single Host"},{"location":"docker/Docker_Clusters/#cluster","text":"If we were to launch a secon container with the same VIRTUAL_HOST, then nginx-proxy will configure the system in a round-robin load balancing scenario: The first request is sent to the first container The second request is sent to the second container # Launch a second container using the same command as before docker run -d -p 80 -e VIRTUAL_HOST=proxy.example katacoda/docker-http-server # If we repeatedly send requests, we'll see them being alternativly handled by the two containers curl http://docker","title":"Cluster"},{"location":"docker/Docker_Clusters/#generated-nginx-configuration","text":"While nginx-proxy will automatically create and configure NGINX for us, we can see the final configuration that it made: docker exec nginx cat /etc/nginx/conf.d/default.conf","title":"Generated NGINX Configuration"},{"location":"docker/Docker_Clusters/#docker-swarm","text":"We'll learn how to initialize a cluster in Docker Swarm Mode and deploy networked containers using the built-in Docker Orchestration . Swarm Mode enables the ability to deploy containers across multiple Docker hosts . It uses overlay networks for service discovery and a built-in load balancer for scaling the services.","title":"Docker Swarm"},{"location":"docker/Docker_Clusters/#key-concepts","text":"Node : A Node is an instance of the Docker Engine connected to the swarm. Nodes are either managers or workers . Managers schedule which containers to run and where Workers execute the tasks Services : A Service is a high-level concept relating to a collection of tasks to be executed by workers. An example of a service is an HTTP server running as a Docker Container on three nodes. Load Balancing : Docker includes a built-in load balancer to process requests across all containers.","title":"Key Concepts"},{"location":"docker/Docker_Clusters/#initialize-swarm-mode","text":"By default, Docker works as an isolated single-node. Swarm Mode turns it into a multi-host cluster-aware engine. The first node to initialize the Swarm Mode becomes the Manager . As new nodes join, they can adjust their roles between managers or workers. You should run 3 to 5 managers in production to ensure high availablility # You can find an overview of the commands with --help $docker swarm --help # Create a Swarm Mode Cluster $docker swarm init Swarm initialized: current node (dy72cua9m3ruay1ebxelb1un2) is now a manager. To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-53u343zuq4nvb66b2qj62deyddj4q03ozj5npqu8fp36ag8tji-adckignv2e49vcx4vkjr8jjjr 172.17.0.41:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.","title":"Initialize Swarm Mode"},{"location":"docker/Docker_Clusters/#join-the-cluster","text":"If some nodes crash, the containers which were running on those hosts will be automatically rescheduled onto other available nodes, ensuring high-availability. Docker uses an additional port 2377 for managing the Swarm. This port should be blocked from public access, and only accessed by trusted users and nodes. Let's get the token that's required to add a worker to the cluster. In this demo, we'll get it with swarm join-token , but in production, this token should be stored securely and only given to trusted individuals . # Ask the manager what the token is token=$(ssh -o StrictHostKeyChecking=no 172.17.0.41 \"docker swarm join-token -q worker\") && echo $token # On the second host, join the cluster docker swarm join 172.17.0.41:2377 --token $token # Now we can view all nodes in the cluster docker node ls","title":"Join the Cluster"},{"location":"docker/Docker_Clusters/#create-an-overlay-network","text":"In previous versions, Docker required an external key-value store (like Consul) to ensure consistency across the network. Now this functionality is incorporated internally into docker. The syntax is the same as before, but we add the overlay keyword when creating the network. docker network create -d overlay skynet","title":"Create an Overlay Network"},{"location":"docker/Docker_Clusters/#deploy-a-service","text":"By default, docker uses a Spread Replication Model to decide which containers should run on which hosts. The spread approach ensures that containers are deployed evenly across the cluster . If a node is removed from the cluster, the workload is rescheduled across the remaining nodes . In this example, we're deploying the image katacoda/docker-http-server with the name http . It will be attached to the skynet network that we just created. docker service create --name http --network skynet For replication and availability, we'll run two instances of replicas of the container across the cluster. --replicas 2 Finally, we load balance the containers together on port 80. If we send an HTTP request to any of the nodes, the cluster will decide which node to respond on. The node which accepted the request may be different from the node where the container responds. In full: # Create the \"http\" service on the \"skynet\" network docker service create --name http --network skynet --replicas 2 -p 80:80 katacoda/docker-http-server # View the currently-running services docker service ls # If we make repeated HTTP requests, we'll see alternate hosts handling the requests curl host01","title":"Deploy a Service"},{"location":"docker/Docker_Clusters/#inspect-state","text":"We can use the Service concept to inspect the health and state of the cluster. # View the list of all tasks associated with the \"http\" service docker service ps http # View the details of a particular service docker service inspect --pretty http # On each node, we can we what task we're currently running # \"self\" refers to the manager node Leader docker node ps self # We can use the ID of a node to query individual hosts docker node ps $(docker node ls -q | head -n1)","title":"Inspect State"},{"location":"docker/Docker_Clusters/#scale-the-service","text":"A Service allows us to scale the number of instances of a task that are running across the cluster. # Run the http service across 5 containers docker service scale http=5 # We can see the new nodes on each host docker ps up","title":"Scale the Service"},{"location":"docker/Docker_Compose/","text":"Docker Compose Docker Compose Docs Docker compose is configured using the YAML file docker-compose.yml . It defines all of the containers and settings needed to launch the set of clusters. The properties map to the way you use the docker run commands, but they're now stored in source control alongside the code. The formate is based on YAML: container_name: property: value - or options Defining a container We want a Node.js application which connects to Redis. We'll start by defining the docker-compose.yml to launch the Node.js application. We'll name the container \"web\", and set the build property to the current directory. This is similar to the commands in docker build -t web . web: build: . If we want to link containers together, we'll specify a links property . This example would link the redis source container (defined in the same file) and assign the same name to the alias. links: - redis The same format is used for other properties, such as the ports property . ports: - \"3000\" - \"8000\" Defining a second container Previously, we used the Dockerfile in the current directory as the base for our container. In this step, we'll use an existing image from Docker Hub as a second container. We can simply add to the same YAML file, using the same format as before. Here, we'll specify the image property and the volumes property . redis: image: redis:alpine volumes: - /var/redis/data:/data Docker Up At this stage, our compose file has the following contents: web: build: . ports: - \"3001\" links: - redis redis: image: redis:alpine volumes: - /var/redis/data:/data Now that we have a docker-compose.yml file, we can launch all of the defined applications with the up command: # The \"-d\" flag has the same result as it does in \"docker run\" # It causes the containers to be run in the background docker-compose up -d If you want to run a single container, you could use docker-compose up <name> Docker Management Docker Compose can also be used to manage all containers easily. # Check the details of the launched containers docker-compose ps # Access all of the logs via a single stream docker-compose logs # Checkout the other available commands by running docker-compose Docker Scale Docker Compose understands how to launch the application containers, so it can also be used to scale the number of running containers . The scale option allows you to specify the service you want, then the number of instances of that service. # Scale the number of containers we want running our \"web\" service docker-compose scale web=3 # You can also scale back down docker compose scale web=1 Stopping Containers We can stop and remove containers with the same intuition we used in starting them: # Stop a set of containers docker-compose stop # Remove all containers docker-compose rm up","title":"Docker Compose"},{"location":"docker/Docker_Compose/#docker-compose","text":"Docker Compose Docs Docker compose is configured using the YAML file docker-compose.yml . It defines all of the containers and settings needed to launch the set of clusters. The properties map to the way you use the docker run commands, but they're now stored in source control alongside the code. The formate is based on YAML: container_name: property: value - or options","title":"Docker Compose"},{"location":"docker/Docker_Compose/#defining-a-container","text":"We want a Node.js application which connects to Redis. We'll start by defining the docker-compose.yml to launch the Node.js application. We'll name the container \"web\", and set the build property to the current directory. This is similar to the commands in docker build -t web . web: build: . If we want to link containers together, we'll specify a links property . This example would link the redis source container (defined in the same file) and assign the same name to the alias. links: - redis The same format is used for other properties, such as the ports property . ports: - \"3000\" - \"8000\"","title":"Defining a container"},{"location":"docker/Docker_Compose/#defining-a-second-container","text":"Previously, we used the Dockerfile in the current directory as the base for our container. In this step, we'll use an existing image from Docker Hub as a second container. We can simply add to the same YAML file, using the same format as before. Here, we'll specify the image property and the volumes property . redis: image: redis:alpine volumes: - /var/redis/data:/data","title":"Defining a second container"},{"location":"docker/Docker_Compose/#docker-up","text":"At this stage, our compose file has the following contents: web: build: . ports: - \"3001\" links: - redis redis: image: redis:alpine volumes: - /var/redis/data:/data Now that we have a docker-compose.yml file, we can launch all of the defined applications with the up command: # The \"-d\" flag has the same result as it does in \"docker run\" # It causes the containers to be run in the background docker-compose up -d If you want to run a single container, you could use docker-compose up <name>","title":"Docker Up"},{"location":"docker/Docker_Compose/#docker-management","text":"Docker Compose can also be used to manage all containers easily. # Check the details of the launched containers docker-compose ps # Access all of the logs via a single stream docker-compose logs # Checkout the other available commands by running docker-compose","title":"Docker Management"},{"location":"docker/Docker_Compose/#docker-scale","text":"Docker Compose understands how to launch the application containers, so it can also be used to scale the number of running containers . The scale option allows you to specify the service you want, then the number of instances of that service. # Scale the number of containers we want running our \"web\" service docker-compose scale web=3 # You can also scale back down docker compose scale web=1","title":"Docker Scale"},{"location":"docker/Docker_Compose/#stopping-containers","text":"We can stop and remove containers with the same intuition we used in starting them: # Stop a set of containers docker-compose stop # Remove all containers docker-compose rm up","title":"Stopping Containers"},{"location":"docker/Docker_Overview/","text":"Docker Containers are a paradigm for application development and deployment. Containers use Linux Kernel features to run applications in an isolated way on a shared machine namespaces cgroups capabilities Application source code, Docker build info, and automation code should be stored together on github. Commands # Find existing images on the Docker registry docker search <image> # List all running containers docker ps # Get more details about a container # docker inspect <name|id> # Display messages the container has written to stderr or stdout docker logs <name|id> Formatting Output from docker ps The standard docker ps command outputs the name, image used, command, uptime, and port info. It's output in a tabular format: CONTAINER ID | IMAGE | COMMAND | CREATED | STATUS | PORTS | NAMES ---|---|---|---|---|---|--- a05ef43766d1 | redis | \"docker-entrypoint.s\u2026\" | 4 minutes ago | Up 4 minutes | 6379/tcp | pedantic_ride We can limit which columns are displayed using the --format parameter. The pretty-printing is configured using a ( Go Template ) syntax: $docker ps --format '{{.Names}} container is using {{.Image}} image' pedantic_ride container is using redis image # And since it's using Go templates, it includes helper functions like \"table\" $docker ps --format 'table {{.Names}}\\t{{.Image}}' NAMES IMAGE pedantic_ride redis Adding Docker Inspect The docker ps command doesn't give us all the information about a container. We'll also want to be able to display info from docker inspect . The output from inspect is in a JSON format, but we can use the same syntax # Note the multi-level JSON parsing with .NetworkSettings.IPAddress docker ps -q | xargs docker inspect --format '{{ .Id }} - {{.Name}} - {{ .NetworkSettings.IPAddress }}' Ports Containers are in a sandbox if a service must be accessible by a process outside the container, we'll need to expose the port via the host ports are bound when the container starts using -p <host-port>:<container-port> docker run -d --name redisHostPort -p 6379:6379 redis:latest By default, the port on host is mapped to 0.0.0.0 which means all ip-addresses . You can specify a specific ip with -p 127.0.0.1:6379:6379 If you run a process on a fixed-port then you can only run one instance! You can randomly assign one and discover it instead with port <name> : docker run -d --name redisDynamic -p 6379 redis:latest docker port redisDynamic 6379 Persistent Data Containers are designed to be stateless . You can bind directories (volumes): -v <host-dir>:<container:dir> . This way, the data is stored on the host, and is persistent. For example, say we know that Redis stores logs and data to the /data directory. We should map this to the host as /opt/docker/data/redis : docker run -d --name redisMapped -v /opt/docker/data/redis:/data redis Foreground & Background Containers Previously, we've run docker with the -d flag, meaning \"run this in the background as a detatched container\". We can use -it instead to get an interactive terminal . docker run -it ubuntu bash Environment Variables Docker images should be built to be transferable between environments without issues or rebuilding. Environment variables can be defined when you launch a container. docker run -d --name my_production_running_app -e NODE_ENV=production -p 3000:3000 my-nodejs-app Docker Stats When you're running production containers, it's important to monitor the runtime metrics like CPU and memory usage. Docker has some built in tools for this purpose Single Container Say we have an existing running container with the name nginx . We can check its stats with the stats command: # This will provide live data from the container docker stats nginx Multiple Containers If we want to view the stats for multiple containers, we can use xargs. We'll get the list of running containers with docker ps , then pipe them as arguments to docker stats . docker ps -q | xargs docker stats up","title":"Overview"},{"location":"docker/Docker_Overview/#docker","text":"Containers are a paradigm for application development and deployment. Containers use Linux Kernel features to run applications in an isolated way on a shared machine namespaces cgroups capabilities Application source code, Docker build info, and automation code should be stored together on github.","title":"Docker"},{"location":"docker/Docker_Overview/#commands","text":"# Find existing images on the Docker registry docker search <image> # List all running containers docker ps # Get more details about a container # docker inspect <name|id> # Display messages the container has written to stderr or stdout docker logs <name|id>","title":"Commands"},{"location":"docker/Docker_Overview/#formatting-output-from-docker-ps","text":"The standard docker ps command outputs the name, image used, command, uptime, and port info. It's output in a tabular format: CONTAINER ID | IMAGE | COMMAND | CREATED | STATUS | PORTS | NAMES ---|---|---|---|---|---|--- a05ef43766d1 | redis | \"docker-entrypoint.s\u2026\" | 4 minutes ago | Up 4 minutes | 6379/tcp | pedantic_ride We can limit which columns are displayed using the --format parameter. The pretty-printing is configured using a ( Go Template ) syntax: $docker ps --format '{{.Names}} container is using {{.Image}} image' pedantic_ride container is using redis image # And since it's using Go templates, it includes helper functions like \"table\" $docker ps --format 'table {{.Names}}\\t{{.Image}}' NAMES IMAGE pedantic_ride redis","title":"Formatting Output from docker ps"},{"location":"docker/Docker_Overview/#adding-docker-inspect","text":"The docker ps command doesn't give us all the information about a container. We'll also want to be able to display info from docker inspect . The output from inspect is in a JSON format, but we can use the same syntax # Note the multi-level JSON parsing with .NetworkSettings.IPAddress docker ps -q | xargs docker inspect --format '{{ .Id }} - {{.Name}} - {{ .NetworkSettings.IPAddress }}'","title":"Adding Docker Inspect"},{"location":"docker/Docker_Overview/#ports","text":"Containers are in a sandbox if a service must be accessible by a process outside the container, we'll need to expose the port via the host ports are bound when the container starts using -p <host-port>:<container-port> docker run -d --name redisHostPort -p 6379:6379 redis:latest By default, the port on host is mapped to 0.0.0.0 which means all ip-addresses . You can specify a specific ip with -p 127.0.0.1:6379:6379 If you run a process on a fixed-port then you can only run one instance! You can randomly assign one and discover it instead with port <name> : docker run -d --name redisDynamic -p 6379 redis:latest docker port redisDynamic 6379","title":"Ports"},{"location":"docker/Docker_Overview/#persistent-data","text":"Containers are designed to be stateless . You can bind directories (volumes): -v <host-dir>:<container:dir> . This way, the data is stored on the host, and is persistent. For example, say we know that Redis stores logs and data to the /data directory. We should map this to the host as /opt/docker/data/redis : docker run -d --name redisMapped -v /opt/docker/data/redis:/data redis","title":"Persistent Data"},{"location":"docker/Docker_Overview/#foreground-background-containers","text":"Previously, we've run docker with the -d flag, meaning \"run this in the background as a detatched container\". We can use -it instead to get an interactive terminal . docker run -it ubuntu bash","title":"Foreground &amp; Background Containers"},{"location":"docker/Docker_Overview/#environment-variables","text":"Docker images should be built to be transferable between environments without issues or rebuilding. Environment variables can be defined when you launch a container. docker run -d --name my_production_running_app -e NODE_ENV=production -p 3000:3000 my-nodejs-app","title":"Environment Variables"},{"location":"docker/Docker_Overview/#docker-stats","text":"When you're running production containers, it's important to monitor the runtime metrics like CPU and memory usage. Docker has some built in tools for this purpose","title":"Docker Stats"},{"location":"docker/Docker_Overview/#single-container","text":"Say we have an existing running container with the name nginx . We can check its stats with the stats command: # This will provide live data from the container docker stats nginx","title":"Single Container"},{"location":"docker/Docker_Overview/#multiple-containers","text":"If we want to view the stats for multiple containers, we can use xargs. We'll get the list of running containers with docker ps , then pipe them as arguments to docker stats . docker ps -q | xargs docker stats up","title":"Multiple Containers"},{"location":"docker/Stateful_Data_Containers/","text":"Stateful Docker Containers There are two ways to approach stateful containers: Mount a volume with -v <host-dir>:<container-dir> Create Data Containers Data Containers A Data Container is only used to store and manage data. We'll use busybox as a lightweight base, and supply the -v option to define where other containers will read and store data. docker create -v /config --name dataContainer busybox Next we can copy our files into the container with cp . We'll copy some file \"config.conf\" into our data container's /config directory. docker cp config.conf dataContainer:/config/ When we're starting other containers, we can reference this dataContainer with the --volumes-from flag. docker run --volumes-from dataContainer ubuntu ls /config up","title":"Statefule Data Containers"},{"location":"docker/Stateful_Data_Containers/#stateful-docker-containers","text":"There are two ways to approach stateful containers: Mount a volume with -v <host-dir>:<container-dir> Create Data Containers","title":"Stateful Docker Containers"},{"location":"docker/Stateful_Data_Containers/#data-containers","text":"A Data Container is only used to store and manage data. We'll use busybox as a lightweight base, and supply the -v option to define where other containers will read and store data. docker create -v /config --name dataContainer busybox Next we can copy our files into the container with cp . We'll copy some file \"config.conf\" into our data container's /config directory. docker cp config.conf dataContainer:/config/ When we're starting other containers, we can reference this dataContainer with the --volumes-from flag. docker run --volumes-from dataContainer ubuntu ls /config up","title":"Data Containers"}]}